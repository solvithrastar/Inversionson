inversion_path = "{INVERSION_PATH}"
lasif_root = "{INVERSION_PATH}/LASIF_PROJECT"
# Pick between 'multi-mesh' or 'mono-mesh'
meshes = "multi-mesh"
# Currently only 'Adam' and 'SGDM' available
optimizer = "Adam"
inversion_parameters = [ "VPV", "VPH", "VSV", "VSH", "RHO",]
modelling_parameters = [ "VPV", "VPH", "VSV", "VSH", "RHO", "QKAPPA", "QMU", "ETA",]
batch_size = 3
cut_source_region_from_gradient_in_km = 100.0
# Values between 0.55 - 1.0. The number represents the quantile where the gradient will be clipped. If 1.0 nothing will be cut
clip_gradient = 1.0
# You specify the length of the absorbing boundaries in the lasif config
absorbing_boundaries = true

[Meshing]
# Only used for multi-mesh. Needs to be higher for more complex models.
elements_per_azimuthal_quarter = 4
elements_per_wavelength = 1.7
ellipticity = true

[Meshing.ocean_loading]
use = false
file = ""
remote_path = ""
variable = ""

[Meshing.topography]
use = false
file = ""
remote_path = ""
variable = ""

[inversion_monitoring]
# If you want to check misfit of validation set every few iterations you give a number N here if you want to do a validation check every N iterations. Put 0 if you do not want to perform a validation check.
iterations_between_validation_checks = 0
# Set to true if you want to use the average of models between validation checks.
use_model_averaging = false
# A validation dataset is used to monitor state of inversion, it is used to tune parameters such as smoothing lengths. It is not used in the inversion in any other way. This parameter is optional. If you keep the above parameter at zero, you can still reserve sources for validation without it happening automatically.
validation_dataset = []
# A test dataset is not used at all in the inversion and can only be used at the end of an inversion to test how reliable the inversion actually was. The only thing Inveversionson does with these events is to make sure they are not used in inversion. This parameter is optional.
test_dataset = []

[HPC]
# Fast directory where inversionson can keep files that need to be used often. Same place as where your jobs are run.
inversionson_fast_dir = "scratch/snx3000/username/INVERSIONSON"
# It's possible to upload meshes to this directory and then they will always be used rather than creating one.
remote_mesh_directory = "path_to_directory_containing_meshes_and_models"
# This conda environment will be activated before running the interpolation and hpc processing jobs.
remote_conda_environment = "salvus"
# Waiting time to check again if jobs are finished. Most HPC clusters don't like it if you check too often.
sleep_time_in_seconds = 20
# Maximum number of reposts if a job fails.
max_reposts = 3

[HPC.wave_propagation]
site_name = "daint"
wall_time = 3600
ranks = 48

# It is possible to process the data on the HPC. In this case, you have to provide the path to the data folder
# and ensure that the events that are available locally are also on the remote. Inversionson
# expects the same structure as in lasif_project/DATA/EARTHQUAKES. One event per file.
# Currently, this is only be supported for the multi-mesh workflow.
# The walltime is added to the model interpolation job if Inversionson detects that the processed data does not exist
# on the remote yet.
[HPC.remote_data_processing]
use = false
remote_raw_data_directory = "path_to_remote_data_directory"
wall_time = 600

[HPC.diffusion_equation]
wall_time = 1000
ranks = 24

[HPC.interpolation]
model_wall_time = 300
gradient_wall_time = 900

# Setting the below value to true makes inversionson select windows, compute station weights
# and compute adjoint sources on the HPC. This is required when using inversionson in combination with
# a remote raw data storage. It currently uses tf-phase misfits and the default LASIF settings for window selection.
[HPC.processing]
use = true
wall_time = 1000
